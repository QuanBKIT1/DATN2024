{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import lib (C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'TEST (Python 3.10.9)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import yaml\n",
    "import json\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pprint\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pre_processing import preprocess\n",
    "from post_processing import postprocess\n",
    "from inference import inference_batch_imgs\n",
    "import configs\n",
    "from utils import import_class, build_session, prepare_data, draw_skeleton\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "# %matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file (C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'TEST (Python 3.10.9)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# Read file for mapping (video_id, label, gloss)\n",
    "json_path = configs.WLASL_JSON_PATH\n",
    "\n",
    "wlasl_class_list_path = configs.WLASL_CLASS_LIST_PATH\n",
    "\n",
    "\n",
    "with open(json_path, 'r') as f:\n",
    "    wlasl2000_json = json.load(f)  # Mapping video id to label\n",
    "\n",
    "\n",
    "with open(wlasl_class_list_path, mode='r') as f:\n",
    "    lines = f.readlines()\n",
    "    label_to_gloss = dict()\n",
    "    for line in lines:\n",
    "        data = line.split('\\t')\n",
    "        label = int(data[0])\n",
    "        gloss = data[1].rstrip()\n",
    "        label_to_gloss[label] = gloss       # Mapping label to gloss\n",
    "\n",
    "# # Read video data path\n",
    "# input_dirs = [\n",
    "#     r\"D:\\DATN\\project\\data\\raw_data\\rgb\\WLASL2000\\train\\\\\",\n",
    "#     r\"D:\\DATN\\project\\data\\raw_data\\rgb\\WLASL2000\\val\\\\\",\n",
    "#     r\"D:\\DATN\\project\\data\\raw_data\\rgb\\WLASL2000\\test\\\\\",\n",
    "# ]\n",
    "# paths = []\n",
    "# for dir_path in input_dirs:\n",
    "#     path1 = []\n",
    "#     for root, _, fnames in os.walk(dir_path):\n",
    "#         for fname in fnames:\n",
    "#             path0 = os.path.join(root, fname)\n",
    "#             path1.append(path0)\n",
    "#     paths.append(path1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pose estimator model and GCN model (A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'TEST (Python 3.10.9)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# Config area\n",
    "\n",
    "index_GCN = 2\n",
    "index_PE = 0        # index_PE = 1 if index_GCN = 1\n",
    "num_keypoint = 31   # 27 if index_GCN = 0,1 and 31 if index_GCN = 2,3\n",
    "\n",
    "# -------------------------------- POSE ESTIMATOR CONFIG -------------------------------- #\n",
    "# onnx_file = r\"D:\\DATN\\project\\data_prepare\\onnx_model\\rtmw-dw-x-l_simcc-cocktail14_270e-384x288_20231122\\end2end.onnx\"\n",
    "# onnx_file = r\"D:\\DATN\\project\\data_prepare\\onnx_model\\rtmpose-l_simcc-ucoco_dw-ucoco_270e-256x192-4d6dfc62_20230728\\20230831\\rtmpose_onnx\\rtmpose-l_simcc-ucoco_dw-ucoco_270e-256x192-4d6dfc62_20230728\\end2end.onnx\"\n",
    "\n",
    "pe_onnx_paths = configs.PE_ONNX_PATHS\n",
    "pe_onnx_path = pe_onnx_paths[index_PE]\n",
    "device = \"cpu\"\n",
    "\n",
    "# -------------------------------- GCN MODEL CONFIG -------------------------------- #\n",
    "\n",
    "# model_config_path = r'D:\\DATN\\project\\Pose-based-WLASL\\checkpoints\\stgcnpp-27-rtmw_onehot_top1=53.09.yaml'\n",
    "\n",
    "model_config_paths = configs.MODEL_CONFIG_PATHS\n",
    "weight_paths = configs.WEIGHT_PATHS\n",
    "\n",
    "weight_path = weight_paths[index_GCN]\n",
    "model_config_path = model_config_paths[index_GCN]\n",
    "\n",
    "print(pe_onnx_path)\n",
    "print(model_config_path)\n",
    "print(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'TEST (Python 3.10.9)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# # Download HPE ONNX file\n",
    "# import urllib.request\n",
    "# import shutil\n",
    "# import zipfile\n",
    "\n",
    "# if not os.path.exists(r'..\\onnx_model\\rtmpose-l.onnx') and not os.path.exists(r'..\\onnx_model\\rtmw-l+.onnx'):\n",
    "#     os.makedirs('../onnx_model', exist_ok=True)\n",
    "#     rtml_url = 'https://download.openmmlab.com/mmpose/v1/projects/rtmposev1/onnx_sdk/rtmpose-l_simcc-ucoco_dw-ucoco_270e-256x192-4d6dfc62_20230728.zip'\n",
    "#     rtmw_url = 'https://download.openmmlab.com/mmpose/v1/projects/rtmw/onnx_sdk/rtmw-dw-x-l_simcc-cocktail14_270e-384x288_20231122.zip'\n",
    "\n",
    "#     rtml_name = r'..\\onnx_model\\rtmpose-l.zip'\n",
    "#     rtmw_name = r'..\\onnx_model\\rtmw-l+.zip'\n",
    "#     # Download\n",
    "#     print(\"Download RTMPose-l\")\n",
    "#     urllib.request.urlretrieve(rtml_url, rtml_name)\n",
    "#     print(\"Download RTMW\")\n",
    "#     urllib.request.urlretrieve(rtmw_url, rtmw_name)\n",
    "\n",
    "#     # Extract zip file\n",
    "#     with zipfile.ZipFile(rtml_name, 'r') as zip_ref:\n",
    "#         zip_ref.extractall(r'..\\onnx_model\\rtmpose-l')\n",
    "#     with zipfile.ZipFile(rtmw_name, 'r') as zip_ref:\n",
    "#         zip_ref.extractall(r'..\\onnx_model\\rtmw-l+')\n",
    "\n",
    "#     # Copy onnx file\n",
    "    \n",
    "\n",
    "# # Cleaning\n",
    "# rtmpose_folder_path = '..\\onnx_model\\rtmpose-l'\n",
    "# rtmw_folder_path = '..\\onnx_model\\rtmw-l+'\n",
    "# os.remove(rtml_name)\n",
    "# os.remove(rtmw_name)\n",
    "# shutil.rmtree(rtmpose_folder_path)\n",
    "# shutil.rmtree(rtmw_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model from config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'TEST (Python 3.10.9)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# -------------------------------- LOAD POSE ESTIMATOR MODEL -------------------------------- #\n",
    "\n",
    "# Build session with ONNXRuntime\n",
    "sess = build_session(pe_onnx_path, device)\n",
    "print(f\"Load pose estimator from ONNX file: {pe_onnx_path}\")\n",
    "\n",
    "h, w = sess.get_inputs()[0].shape[2:]\n",
    "model_input_size = (w, h)\n",
    "print(f\"Input size of pose estimator ({w},{h})\")\n",
    "\n",
    "\n",
    "# -------------------------------- LOAD GCN MODEL -------------------------------- #\n",
    "\n",
    "with open(model_config_path) as f:\n",
    "    arg = yaml.load(f, Loader=yaml.FullLoader)\n",
    "arg = argparse.Namespace(**arg)\n",
    "\n",
    "Model = import_class(arg.model)\n",
    "model = Model(**arg.model_args)\n",
    "print(f\"Load GCN from config: {model_config_path}\")\n",
    "\n",
    "weights = torch.load(weight_path, map_location='cpu')\n",
    "new_weights = {}\n",
    "for key, value in weights.items():\n",
    "    new_weights[key[7:]] = value\n",
    "\n",
    "model.load_state_dict(new_weights)\n",
    "model.eval()\n",
    "print(f\"Load GCN weight from: {weight_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'TEST (Python 3.10.9)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "pprint.pprint(vars(arg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline prediction from video path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'TEST (Python 3.10.9)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# Sign Language Recognition from video path\n",
    "\n",
    "def predict_offline(video_path, num_keypoint=27):\n",
    "    \"\"\"\n",
    "    Return:\n",
    "    gloss, results, total_frames, pose_time, model_time \n",
    "\n",
    "    gloss: Text for sign\n",
    "    results: Output from pose estimator with shape (T,W,H)\n",
    "    total_frames : length frame of video \n",
    "    pose_time: time for pose estimation\n",
    "    model_time: time for model inference\n",
    "\n",
    "    \"\"\"\n",
    "    print(video_path)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    stack_frame = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "        resized_img, center, scale = preprocess(frame, model_input_size)\n",
    "        stack_frame.append(resized_img)\n",
    "\n",
    "    # Time for pose estimation\n",
    "    stack_frame = np.array(stack_frame, dtype=np.float32)\n",
    "    total_frames = stack_frame.shape[0]\n",
    "    print(f\"{stack_frame.shape[0]} frames\")\n",
    "    # Stack frame shape:  (34, 256, 192, 3)\n",
    "\n",
    "    start_time = time.time()\n",
    "    # inference\n",
    "    outputs = inference_batch_imgs(sess, stack_frame)\n",
    "    # postprocessing\n",
    "    keypoints, scores = postprocess(outputs, model_input_size, center, scale)\n",
    "    pose_time = time.time() - start_time\n",
    "\n",
    "    results = np.concatenate((keypoints, scores[:, :, None]), axis=2)\n",
    "\n",
    "    # keypoints.shape = (56, 133, 2)\n",
    "    # scores.shape = (56,133, )\n",
    "\n",
    "    # Data-prepare for GCN model\n",
    "    data = prepare_data(results, num_keypoint)\n",
    "\n",
    "    # results.shape: (3, 150, 27, 1)\n",
    "    data = np.expand_dims(data, axis=0)\n",
    "\n",
    "    # Predict\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        data = torch.Tensor(data)\n",
    "        predictions = model(data)\n",
    "    model_time = time.time() - start_time\n",
    "\n",
    "    label = torch.argmax(predictions)\n",
    "    gloss = label_to_gloss[int(label)]\n",
    "\n",
    "    print(\"pose time:\", pose_time)\n",
    "    print(\"model inference  time: \", model_time)\n",
    "\n",
    "    return gloss, results, predictions, total_frames, pose_time, model_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'TEST (Python 3.10.9)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# Visualize result\n",
    "\n",
    "def visualize_results(video_path, results, predictions, predict_label, true_label='x',  output_size=(512, 512), display_label=False, save_video=False, replay_fps=15):\n",
    "    # Pipeline: Replay video, draw results to video, print predict label\n",
    "    video_id = os.path.basename(video_path).split('.')[0]\n",
    "\n",
    "    # ----------------------------- Visualize prediction ----------------------------- #\n",
    "    os.makedirs(configs.SAVE_VISUALIZE_DIR, exist_ok=True)\n",
    "\n",
    "    output_save_fig = r\"{}\\score_{}.png\".format(\n",
    "        configs.SAVE_VISUALIZE_DIR,   video_id)\n",
    "\n",
    "    softmax_layer = nn.Softmax(dim=0)\n",
    "    probality_each_class = softmax_layer(torch.tensor(predictions)[0]).numpy()\n",
    "    sort_probality_index = np.argsort(probality_each_class)[-5:]\n",
    "\n",
    "    # Create scores\n",
    "\n",
    "    top_label = sort_probality_index[::-1]\n",
    "    scores = probality_each_class[top_label]\n",
    "    glosses = [label_to_gloss[index] for index in top_label]\n",
    "    print(glosses, scores)\n",
    "    plt.bar(glosses, scores, label='xyz')\n",
    "\n",
    "    # Add labels and title to the chart\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title(f\"Predict video with true label = \\'{true_label}\\'\")\n",
    "    plt.savefig(output_save_fig)\n",
    "    plt.show()\n",
    "\n",
    "    # ----------------------------- Display true label ----------------------------- #\n",
    "    if display_label:\n",
    "        # Create image display predict text\n",
    "        predict_img = np.zeros((300, 300, 3), dtype=np.uint8)\n",
    "        text = predict_label\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 1\n",
    "        color = (255, 255, 255)  # White color in BGR\n",
    "        thickness = 2\n",
    "\n",
    "        text_size = cv2.getTextSize(text, font, font_scale, thickness)[0]\n",
    "        # Center horizontally\n",
    "        text_x = (predict_img.shape[1] - text_size[0]) // 2\n",
    "        # Center vertically\n",
    "        text_y = (predict_img.shape[0] + text_size[1]) // 2\n",
    "        cv2.putText(predict_img, text, (text_x, text_y),\n",
    "                    font, font_scale, color, thickness)\n",
    "        cv2.imshow(\"Predict label\", predict_img)\n",
    "\n",
    "        if true_label != 'x':\n",
    "            # Create image display true text\n",
    "            true_img = np.zeros((300, 300, 3), dtype=np.uint8)\n",
    "            text = true_label\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            font_scale = 1\n",
    "            color = (255, 255, 255)  # White color in BGR\n",
    "            thickness = 2\n",
    "\n",
    "            text_size = cv2.getTextSize(text, font, font_scale, thickness)[0]\n",
    "            text_x = (true_img.shape[1] - text_size[0]\n",
    "                      ) // 2  # Center horizontally\n",
    "            text_y = (true_img.shape[0] + text_size[1]\n",
    "                      ) // 2  # Center vertically\n",
    "            cv2.putText(true_img, text, (text_x, text_y),\n",
    "                        font, font_scale, color, thickness)\n",
    "            cv2.imshow(\"True label\", true_img)\n",
    "\n",
    "    # ----------------------------- Replay video with skeleton ----------------------------- #\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    i = 0\n",
    "\n",
    "    if save_video:\n",
    "        frames = []\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        fps = replay_fps\n",
    "        frame_width = frame_height = 256\n",
    "\n",
    "        output_video_filename = r\"{}\\skeleton_{}.mp4\".format(\n",
    "            configs.SAVE_VISUALIZE_DIR, video_id)\n",
    "        output_video1_filename = r\"{}\\rgb_{}.mp4\".format(\n",
    "            configs.SAVE_VISUALIZE_DIR, video_id)\n",
    "\n",
    "        selected_index = []\n",
    "        for i, result in enumerate(results):\n",
    "            if np.all(result[:, :2] <= 300) and np.all(result[:, :2] >= -50):\n",
    "                selected_index.append(i)\n",
    "                frame = np.zeros((256, 256, 3), dtype=np.uint8)\n",
    "                # Extract frame with not in (x,y)\n",
    "\n",
    "                frame = draw_skeleton(frame, result, num_keypoint)\n",
    "                frames.append(frame)\n",
    "\n",
    "        video_frames = []\n",
    "        i = 0\n",
    "        while (cap.isOpened()):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if i in selected_index:\n",
    "                video_frames.append(frame)\n",
    "            i += 1\n",
    "        print(len(video_frames))\n",
    "\n",
    "        out = cv2.VideoWriter(output_video_filename, fourcc,\n",
    "                              fps, (frame_width, frame_height))\n",
    "        for frame in frames:\n",
    "            out.write(frame)\n",
    "\n",
    "        out1 = cv2.VideoWriter(output_video1_filename, fourcc,\n",
    "                               fps, (frame_width, frame_height))\n",
    "\n",
    "        for frame in video_frames:\n",
    "            out1.write(frame)\n",
    "\n",
    "        out.release()\n",
    "        out1.release()\n",
    "\n",
    "        print(\"Save video to \", output_video_filename)\n",
    "        print(\"Save video to \", output_video1_filename)\n",
    "\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "        i = 0\n",
    "\n",
    "        while (True):\n",
    "            if i == len(frames):\n",
    "                i = 0\n",
    "                continue\n",
    "\n",
    "            frame = frames[i]\n",
    "            video_frame = video_frames[i]\n",
    "\n",
    "            frame = cv2.resize(frame, output_size)\n",
    "            video_frame = cv2.resize(video_frame, output_size)\n",
    "            time.sleep(1 / replay_fps)\n",
    "            cv2.imshow(\"RGB Video\", video_frame)\n",
    "            cv2.imshow(\"Skeleton Video\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "            i += 1\n",
    "\n",
    "    else:\n",
    "        while (cap.isOpened()):\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            if not ret:\n",
    "                i = 0\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "                continue\n",
    "            frame = draw_skeleton(frame, results[i], num_keypoint)\n",
    "            frame = cv2.resize(frame, output_size)\n",
    "            time.sleep(1 / replay_fps)\n",
    "            cv2.imshow(\"Video\", frame)\n",
    "\n",
    "            i += 1\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'TEST (Python 3.10.9)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# 27175, 45721, 57667\n",
    "video_path = os.path.join(configs.VIDEO_DEMO_PATH, '57667.mp4')\n",
    "\n",
    "gloss, results, predictions, total_frames, pose_time, model_time = predict_offline(\n",
    "    video_path, num_keypoint)\n",
    "video_id = os.path.basename(video_path).split('.')[0]\n",
    "\n",
    "true_label = wlasl2000_json[video_id]['action'][0]\n",
    "true_gloss = label_to_gloss[true_label]\n",
    "print(gloss, '   ', true_gloss)\n",
    "visualize_results(video_path, results, predictions,\n",
    "                  gloss, true_gloss, save_video=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference time of model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'TEST (Python 3.10.9)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# # Predict video from WLASL2000 folder\n",
    "# video_paths = paths[2][:20]\n",
    "# memory = []\n",
    "# predict_true = 0\n",
    "# for video_path in video_paths:\n",
    "#     gloss, results, predictions, total_frames, pose_time, model_time = predict_offline(\n",
    "#         video_path, num_keypoint)\n",
    "#     memory.append((total_frames, pose_time, model_time))\n",
    "#     video_id = os.path.basename(video_path).split('.')[0]\n",
    "#     true_label = wlasl2000_json[video_id]['action'][0]\n",
    "#     true_gloss = label_to_gloss[true_label]\n",
    "#     print(gloss, '   ', true_gloss)\n",
    "#     if gloss == true_gloss:\n",
    "#         predict_true += 1\n",
    "\n",
    "# # Time counting\n",
    "# memory = np.array(memory)\n",
    "# sum_up_memory = np.sum(memory, axis=0)\n",
    "# print(\"pose time per frame: \", sum_up_memory[1]/sum_up_memory[0])\n",
    "# print(\"model time per a inference: \", sum_up_memory[2]/sum_up_memory[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
