{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "from pre_processing import preprocess\n",
    "from post_processing import postprocess\n",
    "from inference import inference_batch_imgs\n",
    "import cv2\n",
    "import numpy as np\n",
    "import yaml\n",
    "import argparse\n",
    "from utils import import_class, build_session, prepare_data\n",
    "import torch\n",
    "\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "import threading\n",
    "import queue\n",
    "import time\n",
    "\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read json file\n",
    "json_path = r'D:\\DATN\\project\\data\\preprocessed_data\\keypoints_NLA_WLASL\\2000.json'\n",
    "\n",
    "with open(json_path, 'r') as f:\n",
    "  wlasl2000_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlasl_class_list_path = r'D:\\DATN\\project\\data\\raw_data\\WLASL\\split\\wlasl_class_list.txt'\n",
    "with open(wlasl_class_list_path, mode='r') as f:\n",
    "    lines = f.readlines()\n",
    "    label_to_gloss = dict()\n",
    "    label_to_gloss_array = []\n",
    "    for line in lines:\n",
    "        data = line.split('\\t')\n",
    "        label = int(data[0])\n",
    "        gloss = data[1].rstrip()\n",
    "        label_to_gloss[label] = gloss\n",
    "        label_to_gloss_array.append(gloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Config area\n",
    "# onnx_file = r\"D:\\DATN\\project\\data_prepare\\onnx_model\\rtmw-dw-x-l_simcc-cocktail14_270e-384x288_20231122\\end2end.onnx\"\n",
    "onnx_file = r\"D:\\DATN\\project\\data_prepare\\onnx_model\\rtmpose-l_simcc-ucoco_dw-ucoco_270e-256x192-4d6dfc62_20230728\\20230831\\rtmpose_onnx\\rtmpose-l_simcc-ucoco_dw-ucoco_270e-256x192-4d6dfc62_20230728\\end2end.onnx\"\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192 256\n"
     ]
    }
   ],
   "source": [
    "# Build session with ONNXRuntime\n",
    "sess = build_session(onnx_file, device)\n",
    "h, w = sess.get_inputs()[0].shape[2:]\n",
    "model_input_size = (w, h)\n",
    "print(w,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\DATN\\project\\Pose-based-WLASL\n"
     ]
    }
   ],
   "source": [
    "%cd \"D:\\DATN\\project\\Pose-based-WLASL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GCN model\n",
    "\n",
    "# model_config_path = r'D:\\DATN\\project\\Pose-based-WLASL\\checkpoints\\stgcnpp-27-rtmw_onehot_top1=53.09.yaml' \n",
    "model_config_path = r\"D:\\DATN\\project\\mycode\\best_weight_GCN\\ctr-gcn-27-rtml-256-192 top1=43.22.yaml\"\n",
    "weights_path = r\"D:\\DATN\\project\\mycode\\best_weight_GCN\\ctr-gcn-27-rtml-256-192 top1=43.22.pt\"\n",
    "\n",
    "with open(model_config_path) as f: \n",
    "    arg = yaml.load(f, Loader=yaml.FullLoader)\n",
    "arg = argparse.Namespace(**arg)\n",
    "\n",
    "arg.model_args['graph_args']['layout'] = 'keypoint-27'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model eval\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Model = import_class(arg.model)\n",
    "model = Model(**arg.model_args)\n",
    "\n",
    "\n",
    "weights = torch.load(weights_path, map_location='cpu')\n",
    "\n",
    "new_weights = {}\n",
    "for key, value in weights.items():\n",
    "  new_weights[key[7:]] = value\n",
    "\n",
    "model.load_state_dict(new_weights)\n",
    "model.eval()\n",
    "print(\"Model eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_offline(video_path):\n",
    "    print(video_path)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    stack_frame = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "        resized_img, center, scale = preprocess(frame, model_input_size)\n",
    "        stack_frame.append(resized_img)\n",
    "\n",
    "    # Time for pose estimation\n",
    "    stack_frame = np.array(stack_frame, dtype=np.float32)\n",
    "    total_frames = stack_frame.shape[0] \n",
    "    print(f\"{stack_frame.shape[0]} frames\")\n",
    "    # Stack frame shape:  (34, 256, 192, 3)\n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    # inference\n",
    "    outputs = inference_batch_imgs(sess, stack_frame)\n",
    "    # postprocessing\n",
    "    keypoints, scores = postprocess(outputs, model_input_size, center, scale)\n",
    "    pose_time = time.time() - start_time\n",
    "\n",
    "    results = np.concatenate((keypoints, scores[:,:, None]), axis=2)\n",
    "    \n",
    "    # keypoints.shape = (56, 133, 2)\n",
    "    # scores.shape = (56,133, )\n",
    "\n",
    "\n",
    "    # Data-prepare\n",
    "    data = prepare_data(results, num_keypoint=27)\n",
    "    # results.shape: (3, 150, 27, 1)\n",
    "    data = np.expand_dims(data, axis=0)\n",
    "    \n",
    "    # Predict \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        data = torch.Tensor(data)\n",
    "        predictions =  model(data)\n",
    "    model_time = time.time() - start_time\n",
    "    \n",
    "    label = torch.argmax(predictions)\n",
    "    gloss = label_to_gloss[int(label)]\n",
    "    return  gloss, total_frames, pose_time, model_time  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load keypoint\n",
    "# # data_path = r\"D:\\DATN\\project\\data\\preprocessed_data\\keypoints-rtm-l-384-288\\skeleton-rtm-l-384-288.pkl\"\n",
    "# data_path = \"D:\\DATN\\project\\data\\preprocessed_data\\keypoints-rtm-l-256-192\\skeleton-rtm-l-256-192.pkl\"\n",
    "\n",
    "# with open(data_path,\n",
    "#           mode='rb') as f:\n",
    "#     wlasl_keypoints = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Shape of a sample value: \", wlasl_keypoints[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "input_dirs = [\n",
    "    r\"D:\\dataset\\rgb_WLASL2000\\WLASL2000\\\\\",\n",
    "              ]\n",
    "paths = []\n",
    "for dir_path in input_dirs:\n",
    "    for root, _, fnames in os.walk(dir_path):\n",
    "        for fname in fnames:     \n",
    "            path1 = os.path.join(root, fname) \n",
    "            paths.append(path1)\n",
    "            \n",
    "paths.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\dataset\\rgb_WLASL2000\\WLASL2000\\\\00295.mp4\n",
      "21 frames\n",
      "s     a\n",
      "D:\\dataset\\rgb_WLASL2000\\WLASL2000\\\\00333.mp4\n",
      "52 frames\n",
      "stomach     abdomen\n"
     ]
    }
   ],
   "source": [
    "video_paths = paths[:2]\n",
    "memory = []\n",
    "predict_true = 0\n",
    "for video_path in video_paths:\n",
    "    gloss, total_frames, pose_time, model_time   = predict_offline(video_path)\n",
    "    memory.append((total_frames, pose_time, model_time))\n",
    "    video_id = os.path.basename(video_path).split('.')[0]\n",
    "    true_label = wlasl2000_json[video_id]['action'][0]\n",
    "    true_gloss = label_to_gloss[true_label]\n",
    "    print(gloss ,'   ', true_gloss)\n",
    "    if gloss == true_gloss:\n",
    "        predict_true += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time counting\n",
    "\n",
    "memory = np.array(memory)\n",
    "\n",
    "sum_up_memory = np.sum(memory, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([73.        ,  5.80179787,  0.39365339])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_up_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pose time per frame:  0.07947668310714094\n",
      "model time per a inference:  0.0053925122300239456\n"
     ]
    }
   ],
   "source": [
    "print(\"pose time per frame: \", sum_up_memory[1]/sum_up_memory[0])\n",
    "print(\"model time per a inference: \", sum_up_memory[2]/sum_up_memory[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "\n",
    "# Real-time pose estimator \n",
    "\n",
    "# Define box and image sizes\n",
    "box_width, box_height = 400, 400\n",
    "image_width, image_height = 256, 256\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)  # 0 for default webcam\n",
    "\n",
    "prev_frame_time = 0\n",
    "new_frame_time = 0\n",
    "# font which we will be using to display FPS \n",
    "font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "\n",
    "\n",
    "while True:\n",
    "  # Capture frame-by-frame\n",
    "  ret, frame = cap.read()\n",
    "\n",
    "  # Check if frame is read correctly\n",
    "  if not ret:\n",
    "    print(\"Error: Unable to capture frame\")\n",
    "    break\n",
    "\n",
    "  # Get frame dimensions\n",
    "  frame_height, frame_width, _ = frame.shape\n",
    "\n",
    "  # Calculate top-left corner of the box\n",
    "  top_left_x = max(0, (frame_width - box_width) // 2)\n",
    "  top_left_y = max(0, (frame_height - box_height) // 2)\n",
    "\n",
    "  # Draw the box on the frame\n",
    "  cv2.rectangle(frame, (top_left_x, top_left_y), (top_left_x + box_width, top_left_y + box_height), (0, 255, 0), 2)\n",
    "\n",
    "  # Extract the box region\n",
    "  box_region = frame[top_left_y:top_left_y + box_height, top_left_x:top_left_x + box_width]\n",
    "\n",
    "  # Resize the extracted region\n",
    "  resized_image = cv2.resize(box_region, (image_width, image_height))\n",
    "\n",
    "    # time when we finish processing for this frame \n",
    "  new_frame_time = time.time() \n",
    "\n",
    "  # Calculating the fps \n",
    "  fps = 1/(new_frame_time-prev_frame_time) \n",
    "  prev_frame_time = new_frame_time \n",
    "  fps = round(fps,3) \n",
    "  fps = str(fps) \n",
    "\n",
    "  # putting the FPS count on the frame \n",
    "  cv2.putText(frame, fps, (7, 70), font, 3, (100, 255, 0), 3, cv2.LINE_AA) \n",
    "\n",
    "\n",
    "\n",
    "  # Display the frame with box and resized image\n",
    "  cv2.imshow('Webcam Capture', frame)\n",
    "  cv2.imshow('Extracted and Resized', resized_image)\n",
    "\n",
    "  # Press 'q' to quit\n",
    "  if cv2.waitKey(1) == ord('q'):\n",
    "    break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many frames to skip while inferencing\n",
    "# configuring a higher value will result in better FPS (frames per rate), but accuracy might get impacted\n",
    "SKIP_FRAME_COUNT = 3\n",
    "\n",
    "# analyse the video\n",
    "def analyse_video(pose_detector, lstm_classifier, video_path):\n",
    "    # open the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    # width of image frame\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    # height of image frame\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    # frames per second of the input video\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    # total number of frames in the video\n",
    "    tot_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    # video output codec\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    # extract the file name from video path\n",
    "    file_name = ntpath.basename(video_path)\n",
    "    # video writer\n",
    "    vid_writer = cv2.VideoWriter('res_{}'.format(\n",
    "        file_name), fourcc, 30, (width, height))\n",
    "    # counter\n",
    "    counter = 0\n",
    "    \n",
    "    \n",
    "    # buffer to keep the output of detectron2 pose estimation\n",
    "    buffer_window = []\n",
    "    # start time\n",
    "    start = time.time()    \n",
    "    label = None\n",
    "    # iterate through the video\n",
    "    while True:\n",
    "        # read the frame\n",
    "        ret, frame = cap.read()\n",
    "        # return if end of the video\n",
    "        if ret == False:\n",
    "            break\n",
    "        # make a copy of the frame\n",
    "        img = frame.copy()\n",
    "        if(counter % (SKIP_FRAME_COUNT+1) == 0):             \n",
    "          # predict pose estimation on the frame\n",
    "          outputs = pose_detector(frame)          \n",
    "          # filter the outputs with a good confidence score\n",
    "          persons, pIndicies = filter_persons(outputs)\n",
    "          if len(persons) >= 1:\n",
    "              # pick only pose estimation results of the first person.\n",
    "              # actually, we expect only one person to be present in the video. \n",
    "              p = persons[0]\n",
    "              # draw the body joints on the person body\n",
    "              draw_keypoints(p, img)\n",
    "              # input feature array for lstm\n",
    "              features = []\n",
    "              # add pose estimate results to the feature array\n",
    "              for i, row in enumerate(p):\n",
    "                  features.append(row[0])\n",
    "                  features.append(row[1])\n",
    "\n",
    "              # append the feature array into the buffer\n",
    "              # note that max buffer size is 32 and buffer_window operates in a sliding window fashion\n",
    "              if len(buffer_window) < WINDOW_SIZE:\n",
    "                  buffer_window.append(features)\n",
    "              else:\n",
    "                  # convert input to tensor\n",
    "                  model_input = torch.Tensor(np.array(buffer_window, dtype=np.float32))\n",
    "                  # add extra dimension\n",
    "                  model_input = torch.unsqueeze(model_input, dim=0)\n",
    "                  # predict the action class using lstm\n",
    "                  y_pred = lstm_classifier(model_input)\n",
    "                  prob = F.softmax(y_pred, dim=1)\n",
    "                  # get the index of the max probability\n",
    "                  pred_index = prob.data.max(dim=1)[1]\n",
    "                  # pop the first value from buffer_window and add the new entry in FIFO fashion, to have a sliding window of size 32.\n",
    "                  buffer_window.pop(0)\n",
    "                  buffer_window.append(features)\n",
    "                  label = LABELS[pred_index.numpy()[0]]\n",
    "                  #print(\"Label detected \", label)\n",
    "        \n",
    "        # add predicted label into the frame\n",
    "        if label is not None:\n",
    "            cv2.putText(img, 'Action: {}'.format(label),\n",
    "        (int(width-400), height-50), cv2.FONT_HERSHEY_COMPLEX, 0.9, (102, 255, 255), 2)                  \n",
    "        # increment counter\n",
    "        counter += 1\n",
    "        # write the frame into the result video                    \n",
    "        vid_writer.write(img)\n",
    "        # compute the completion percentage\n",
    "        percentage = int(counter*100/tot_frames)\n",
    "        # return the completion percentage\n",
    "        yield \"data:\" + str(percentage) + \"\\n\\n\"\n",
    "    analyze_done = time.time()\n",
    "    print(\"Video processing finished in \", analyze_done - start)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
