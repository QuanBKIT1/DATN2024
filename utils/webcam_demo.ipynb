{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import lib (C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import yaml\n",
    "import json\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pprint\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pre_processing import preprocess\n",
    "from post_processing import postprocess\n",
    "from inference import inference_batch_imgs\n",
    "import configs\n",
    "from utils import import_class, build_session, prepare_data, draw_skeleton\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file (C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file for mapping (video_id, label, gloss)\n",
    "json_path = configs.WLASL_JSON_PATH\n",
    "\n",
    "wlasl_class_list_path = configs.WLASL_CLASS_LIST_PATH\n",
    "\n",
    "\n",
    "with open(json_path, 'r') as f:\n",
    "    wlasl2000_json = json.load(f)  # Mapping video id to label\n",
    "\n",
    "\n",
    "with open(wlasl_class_list_path, mode='r') as f:\n",
    "    lines = f.readlines()\n",
    "    label_to_gloss = dict()\n",
    "    for line in lines:\n",
    "        data = line.split('\\t')\n",
    "        label = int(data[0])\n",
    "        gloss = data[1].rstrip()\n",
    "        label_to_gloss[label] = gloss       # Mapping label to gloss\n",
    "\n",
    "# # Read video data path\n",
    "# input_dirs = [\n",
    "#     r\"D:\\DATN\\project\\data\\raw_data\\rgb\\WLASL2000\\train\\\\\",\n",
    "#     r\"D:\\DATN\\project\\data\\raw_data\\rgb\\WLASL2000\\val\\\\\",\n",
    "#     r\"D:\\DATN\\project\\data\\raw_data\\rgb\\WLASL2000\\test\\\\\",\n",
    "# ]\n",
    "# paths = []\n",
    "# for dir_path in input_dirs:\n",
    "#     path1 = []\n",
    "#     for root, _, fnames in os.walk(dir_path):\n",
    "#         for fname in fnames:\n",
    "#             path0 = os.path.join(root, fname)\n",
    "#             path1.append(path0)\n",
    "#     paths.append(path1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pose estimator model and GCN model (A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\onnx_model\\rtmw-l+.onnx\n",
      "..\\checkpoints\\stgcnpp-31-rtmw_onehot_top1=54.66.yaml\n",
      "..\\checkpoints\\stgcnpp-31-rtmw_onehot_top1=54.66.pt\n"
     ]
    }
   ],
   "source": [
    "# Config area\n",
    "\n",
    "index_GCN = 2\n",
    "index_PE = 0        # index_PE = 1 if index_GCN = 1\n",
    "num_keypoint = 31   # 27 if index_GCN = 0,1 and 31 if index_GCN = 2,3\n",
    "\n",
    "# -------------------------------- POSE ESTIMATOR CONFIG -------------------------------- #\n",
    "# onnx_file = r\"D:\\DATN\\project\\data_prepare\\onnx_model\\rtmw-dw-x-l_simcc-cocktail14_270e-384x288_20231122\\end2end.onnx\"\n",
    "# onnx_file = r\"D:\\DATN\\project\\data_prepare\\onnx_model\\rtmpose-l_simcc-ucoco_dw-ucoco_270e-256x192-4d6dfc62_20230728\\20230831\\rtmpose_onnx\\rtmpose-l_simcc-ucoco_dw-ucoco_270e-256x192-4d6dfc62_20230728\\end2end.onnx\"\n",
    "\n",
    "pe_onnx_paths = configs.PE_ONNX_PATHS\n",
    "pe_onnx_path = pe_onnx_paths[index_PE]\n",
    "device = \"cpu\"\n",
    "\n",
    "# -------------------------------- GCN MODEL CONFIG -------------------------------- #\n",
    "\n",
    "# model_config_path = r'D:\\DATN\\project\\Pose-based-WLASL\\checkpoints\\stgcnpp-27-rtmw_onehot_top1=53.09.yaml'\n",
    "\n",
    "model_config_paths = configs.MODEL_CONFIG_PATHS\n",
    "weight_paths = configs.WEIGHT_PATHS\n",
    "\n",
    "weight_path = weight_paths[index_GCN]\n",
    "model_config_path = model_config_paths[index_GCN]\n",
    "\n",
    "print(pe_onnx_path)\n",
    "print(model_config_path)\n",
    "print(weight_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model from config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pose estimator from ONNX file: ..\\onnx_model\\rtmw-l+.onnx\n",
      "Input size of pose estimator (288,384)\n",
      "Load GCN from config: ..\\checkpoints\\stgcnpp-31-rtmw_onehot_top1=54.66.yaml\n",
      "Load GCN weight from: ..\\checkpoints\\stgcnpp-31-rtmw_onehot_top1=54.66.pt\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------- LOAD POSE ESTIMATOR MODEL -------------------------------- #\n",
    "\n",
    "# Build session with ONNXRuntime\n",
    "sess = build_session(pe_onnx_path, device)\n",
    "print(f\"Load pose estimator from ONNX file: {pe_onnx_path}\")\n",
    "\n",
    "h, w = sess.get_inputs()[0].shape[2:]\n",
    "model_input_size = (w, h)\n",
    "print(f\"Input size of pose estimator ({w},{h})\")\n",
    "\n",
    "\n",
    "# -------------------------------- LOAD GCN MODEL -------------------------------- #\n",
    "\n",
    "with open(model_config_path) as f:\n",
    "    arg = yaml.load(f, Loader=yaml.FullLoader)\n",
    "arg = argparse.Namespace(**arg)\n",
    "\n",
    "Model = import_class(arg.model)\n",
    "model = Model(**arg.model_args)\n",
    "print(f\"Load GCN from config: {model_config_path}\")\n",
    "\n",
    "weights = torch.load(weight_path, map_location='cpu')\n",
    "new_weights = {}\n",
    "for key, value in weights.items():\n",
    "    new_weights[key[7:]] = value\n",
    "\n",
    "model.load_state_dict(new_weights)\n",
    "model.eval()\n",
    "print(f\"Load GCN weight from: {weight_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Experiment_name': 'st-gcn-pp',\n",
      " 'base_lr': 0.1,\n",
      " 'batch_size': 64,\n",
      " 'eval_interval': 1,\n",
      " 'feeder': 'feeder.feeder.Feeder',\n",
      " 'model': 'model.stgcn_pp.stgcn_pp.Model',\n",
      " 'model_args': {'gcn_adaptive': 'init',\n",
      "                'gcn_with_res': True,\n",
      "                'graph': 'graph.graph.Graph',\n",
      "                'graph_args': {'layout': 'keypoint-31', 'mode': 'spatial'},\n",
      "                'in_channels': 3,\n",
      "                'num_class': 2000,\n",
      "                'tcn_type': 'mstcn'},\n",
      " 'model_saved_dir': '/kaggle/working/output/save_models',\n",
      " 'nesterov': True,\n",
      " 'num_epoch': 100,\n",
      " 'num_worker': 2,\n",
      " 'optimizer': 'SGD',\n",
      " 'phase': 'train',\n",
      " 'print_log': True,\n",
      " 'save_interval': 20,\n",
      " 'show_topk': [1],\n",
      " 'start_epoch': 0,\n",
      " 't_max': 100,\n",
      " 'test_batch_size': 64,\n",
      " 'test_feeder_args': {'data_path': '/kaggle/input/skeleton-31-rtm-l-384-288/skeleton-31-rtm-l-384-288/test_data_joint.npy',\n",
      "                      'label_path': '/kaggle/input/skeleton-31-rtm-l-384-288/skeleton-31-rtm-l-384-288/test_label.pkl',\n",
      "                      'normalization': True,\n",
      "                      'random_mirror': False},\n",
      " 'train_feeder_args': {'data_path': '/kaggle/input/skeleton-31-rtm-l-384-288/skeleton-31-rtm-l-384-288/train_data_joint.npy',\n",
      "                       'is_vector': False,\n",
      "                       'label_path': '/kaggle/input/skeleton-31-rtm-l-384-288/skeleton-31-rtm-l-384-288/train_label.pkl',\n",
      "                       'max_xy': 256,\n",
      "                       'normalization': True,\n",
      "                       'random_choose': True,\n",
      "                       'random_mirror': True,\n",
      "                       'random_mirror_p': 0.5,\n",
      "                       'random_move': True,\n",
      "                       'random_shift': True,\n",
      "                       'window_size': 80},\n",
      " 'val_feeder_args': {'data_path': '/kaggle/input/skeleton-31-rtm-l-384-288/skeleton-31-rtm-l-384-288/val_data_joint.npy',\n",
      "                     'label_path': '/kaggle/input/skeleton-31-rtm-l-384-288/skeleton-31-rtm-l-384-288/val_label.pkl',\n",
      "                     'normalization': True,\n",
      "                     'random_mirror': False},\n",
      " 'weight_decay': 0.0005,\n",
      " 'weights': 0,\n",
      " 'work_dir': '/kaggle/working/output/work_dir'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(vars(arg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline prediction from video path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sign Language Recognition from video path\n",
    "\n",
    "def predict_offline(video_path, num_keypoint=27):\n",
    "    \"\"\"\n",
    "    Return:\n",
    "    gloss, results, total_frames, pose_time, model_time \n",
    "\n",
    "    gloss: Text for sign\n",
    "    results: Output from pose estimator with shape (T,W,H)\n",
    "    total_frames : length frame of video \n",
    "    pose_time: time for pose estimation\n",
    "    model_time: time for model inference\n",
    "\n",
    "    \"\"\"\n",
    "    print(video_path)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    stack_frame = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "        resized_img, center, scale = preprocess(frame, model_input_size)\n",
    "        stack_frame.append(resized_img)\n",
    "\n",
    "    # Time for pose estimation\n",
    "    stack_frame = np.array(stack_frame, dtype=np.float32)\n",
    "    total_frames = stack_frame.shape[0]\n",
    "    print(f\"{stack_frame.shape[0]} frames\")\n",
    "    # Stack frame shape:  (34, 256, 192, 3)\n",
    "\n",
    "    start_time = time.time()\n",
    "    # inference\n",
    "    outputs = inference_batch_imgs(sess, stack_frame)\n",
    "    # postprocessing\n",
    "    keypoints, scores = postprocess(outputs, model_input_size, center, scale)\n",
    "    pose_time = time.time() - start_time\n",
    "\n",
    "    results = np.concatenate((keypoints, scores[:, :, None]), axis=2)\n",
    "\n",
    "    # keypoints.shape = (56, 133, 2)\n",
    "    # scores.shape = (56,133, )\n",
    "\n",
    "    # Data-prepare for GCN model\n",
    "    data = prepare_data(results, num_keypoint)\n",
    "\n",
    "    # results.shape: (3, 150, 27, 1)\n",
    "    data = np.expand_dims(data, axis=0)\n",
    "\n",
    "    # Predict\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        data = torch.Tensor(data)\n",
    "        predictions = model(data)\n",
    "    model_time = time.time() - start_time\n",
    "\n",
    "    label = torch.argmax(predictions)\n",
    "    gloss = label_to_gloss[int(label)]\n",
    "\n",
    "    print(\"pose time:\", pose_time)\n",
    "    print(\"model inference  time: \", model_time)\n",
    "\n",
    "    return gloss, results, predictions, total_frames, pose_time, model_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize result\n",
    "\n",
    "def visualize_results(video_path, results, predictions, predict_label, true_label='x',  output_size=(512, 512), display_label=False, save_video=False, replay_fps=15):\n",
    "    # Pipeline: Replay video, draw results to video, print predict label\n",
    "    video_id = os.path.basename(video_path).split('.')[0]\n",
    "\n",
    "    # ----------------------------- Visualize prediction ----------------------------- #\n",
    "    os.makedirs(configs.SAVE_VISUALIZE_DIR, exist_ok=True)\n",
    "\n",
    "    output_save_fig = r\"{}\\score_{}.png\".format(\n",
    "        configs.SAVE_VISUALIZE_DIR,   video_id)\n",
    "\n",
    "    softmax_layer = nn.Softmax(dim=0)\n",
    "    probality_each_class = softmax_layer(torch.tensor(predictions)[0]).numpy()\n",
    "    sort_probality_index = np.argsort(probality_each_class)[-5:]\n",
    "\n",
    "    # Create scores\n",
    "\n",
    "    top_label = sort_probality_index[::-1]\n",
    "    scores = probality_each_class[top_label]\n",
    "    glosses = [label_to_gloss[index] for index in top_label]\n",
    "    print(glosses, scores)\n",
    "    plt.bar(glosses, scores, label='xyz')\n",
    "\n",
    "    # Add labels and title to the chart\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title(f\"Predict video with true label = \\'{true_label}\\'\")\n",
    "    plt.savefig(output_save_fig)\n",
    "    plt.show()\n",
    "\n",
    "    # ----------------------------- Display true label ----------------------------- #\n",
    "    if display_label:\n",
    "        # Create image display predict text\n",
    "        predict_img = np.zeros((300, 300, 3), dtype=np.uint8)\n",
    "        text = predict_label\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 1\n",
    "        color = (255, 255, 255)  # White color in BGR\n",
    "        thickness = 2\n",
    "\n",
    "        text_size = cv2.getTextSize(text, font, font_scale, thickness)[0]\n",
    "        # Center horizontally\n",
    "        text_x = (predict_img.shape[1] - text_size[0]) // 2\n",
    "        # Center vertically\n",
    "        text_y = (predict_img.shape[0] + text_size[1]) // 2\n",
    "        cv2.putText(predict_img, text, (text_x, text_y),\n",
    "                    font, font_scale, color, thickness)\n",
    "        cv2.imshow(\"Predict label\", predict_img)\n",
    "\n",
    "        if true_label != 'x':\n",
    "            # Create image display true text\n",
    "            true_img = np.zeros((300, 300, 3), dtype=np.uint8)\n",
    "            text = true_label\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            font_scale = 1\n",
    "            color = (255, 255, 255)  # White color in BGR\n",
    "            thickness = 2\n",
    "\n",
    "            text_size = cv2.getTextSize(text, font, font_scale, thickness)[0]\n",
    "            text_x = (true_img.shape[1] - text_size[0]\n",
    "                      ) // 2  # Center horizontally\n",
    "            text_y = (true_img.shape[0] + text_size[1]\n",
    "                      ) // 2  # Center vertically\n",
    "            cv2.putText(true_img, text, (text_x, text_y),\n",
    "                        font, font_scale, color, thickness)\n",
    "            cv2.imshow(\"True label\", true_img)\n",
    "\n",
    "    # ----------------------------- Replay video with skeleton ----------------------------- #\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    i = 0\n",
    "\n",
    "    if save_video:\n",
    "        frames = []\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        fps = replay_fps\n",
    "        frame_width = frame_height = 256\n",
    "\n",
    "        output_video_filename = r\"{}\\skeleton_{}.mp4\".format(\n",
    "            configs.SAVE_VISUALIZE_DIR, video_id)\n",
    "        output_video1_filename = r\"{}\\rgb_{}.mp4\".format(\n",
    "            configs.SAVE_VISUALIZE_DIR, video_id)\n",
    "\n",
    "        selected_index = []\n",
    "        for i, result in enumerate(results):\n",
    "            if np.all(result[:, :2] <= 300) and np.all(result[:, :2] >= -50):\n",
    "                selected_index.append(i)\n",
    "                frame = np.zeros((256, 256, 3), dtype=np.uint8)\n",
    "                # Extract frame with not in (x,y)\n",
    "\n",
    "                frame = draw_skeleton(frame, result, num_keypoint)\n",
    "                frames.append(frame)\n",
    "\n",
    "        video_frames = []\n",
    "        i = 0\n",
    "        while (cap.isOpened()):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if i in selected_index:\n",
    "                video_frames.append(frame)\n",
    "            i += 1\n",
    "        print(len(video_frames))\n",
    "\n",
    "        out = cv2.VideoWriter(output_video_filename, fourcc,\n",
    "                              fps, (frame_width, frame_height))\n",
    "        for frame in frames:\n",
    "            out.write(frame)\n",
    "\n",
    "        out1 = cv2.VideoWriter(output_video1_filename, fourcc,\n",
    "                               fps, (frame_width, frame_height))\n",
    "\n",
    "        for frame in video_frames:\n",
    "            out1.write(frame)\n",
    "\n",
    "        out.release()\n",
    "        out1.release()\n",
    "\n",
    "        print(\"Save video to \", output_video_filename)\n",
    "        print(\"Save video to \", output_video1_filename)\n",
    "\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "        i = 0\n",
    "\n",
    "        while (True):\n",
    "            if i == len(frames):\n",
    "                i = 0\n",
    "                continue\n",
    "\n",
    "            frame = frames[i]\n",
    "            video_frame = video_frames[i]\n",
    "\n",
    "            frame = cv2.resize(frame, output_size)\n",
    "            video_frame = cv2.resize(video_frame, output_size)\n",
    "            time.sleep(1 / replay_fps)\n",
    "            cv2.imshow(\"RGB Video\", video_frame)\n",
    "            cv2.imshow(\"Skeleton Video\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "            i += 1\n",
    "\n",
    "    else:\n",
    "        while (cap.isOpened()):\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            if not ret:\n",
    "                i = 0\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "                continue\n",
    "            frame = draw_skeleton(frame, results[i], num_keypoint)\n",
    "            frame = cv2.resize(frame, output_size)\n",
    "            time.sleep(1 / replay_fps)\n",
    "            cv2.imshow(\"Video\", frame)\n",
    "\n",
    "            i += 1\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\data\\video_demo\\57667.mp4\n",
      "66 frames\n",
      "pose time: 14.761641263961792\n",
      "model inference  time:  0.40990424156188965\n",
      "thank you     thank you\n",
      "['thank you', 'thankful', 'good', 'sweet', 'wolf'] [9.9364966e-01 1.4982306e-03 7.7855081e-04 5.2938395e-04 4.5979861e-04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Quan\\AppData\\Local\\Temp\\ipykernel_25488\\1356431405.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  probality_each_class = softmax_layer(torch.tensor(predictions)[0]).numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "Save video to  ../visualize_tmp\\skeleton_57667.mp4\n",
      "Save video to  ../visualize_tmp\\rgb_57667.mp4\n"
     ]
    }
   ],
   "source": [
    "# 27175, 45721, 57667\n",
    "video_path = os.path.join(configs.VIDEO_DEMO_PATH, '57667.mp4')\n",
    "\n",
    "gloss, results, predictions, total_frames, pose_time, model_time = predict_offline(\n",
    "    video_path, num_keypoint)\n",
    "video_id = os.path.basename(video_path).split('.')[0]\n",
    "\n",
    "true_label = wlasl2000_json[video_id]['action'][0]\n",
    "true_gloss = label_to_gloss[true_label]\n",
    "print(gloss, '   ', true_gloss)\n",
    "visualize_results(video_path, results, predictions,\n",
    "                  gloss, true_gloss, save_video=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference time of model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict video from WLASL2000 folder\n",
    "# video_paths = paths[2][:20]\n",
    "# memory = []\n",
    "# predict_true = 0\n",
    "# for video_path in video_paths:\n",
    "#     gloss, results, predictions, total_frames, pose_time, model_time = predict_offline(\n",
    "#         video_path, num_keypoint)\n",
    "#     memory.append((total_frames, pose_time, model_time))\n",
    "#     video_id = os.path.basename(video_path).split('.')[0]\n",
    "#     true_label = wlasl2000_json[video_id]['action'][0]\n",
    "#     true_gloss = label_to_gloss[true_label]\n",
    "#     print(gloss, '   ', true_gloss)\n",
    "#     if gloss == true_gloss:\n",
    "#         predict_true += 1\n",
    "\n",
    "# # Time counting\n",
    "# memory = np.array(memory)\n",
    "# sum_up_memory = np.sum(memory, axis=0)\n",
    "# print(\"pose time per frame: \", sum_up_memory[1]/sum_up_memory[0])\n",
    "# print(\"model time per a inference: \", sum_up_memory[2]/sum_up_memory[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
