{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import configs\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FastText pretrained in Common Crawl with model name (crawl-300d-2M-subword)\n",
    "pre_ft_vec_path = configs.FASTTEXT_PRETRAINED_PATH\n",
    "model = KeyedVectors.load_word2vec_format(pre_ft_vec_path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'configs' has no attribute 'WLASL_CLASS_LIST_PATH'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Get list of word\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m list_word_path \u001b[38;5;241m=\u001b[39m \u001b[43mconfigs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWLASL_CLASS_LIST_PATH\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(list_word_path, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      5\u001b[0m     lines \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'configs' has no attribute 'WLASL_CLASS_LIST_PATH'"
     ]
    }
   ],
   "source": [
    "# Get list of word\n",
    "list_word_path = configs.WLASL_CLASS_LIST_PATH\n",
    "\n",
    "with open(list_word_path, mode='r') as f:\n",
    "    lines = f.readlines()\n",
    "    list_word = []\n",
    "    for line in lines:\n",
    "        word = line.split(\"\\t\")[1].strip()\n",
    "        list_word.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163 losear\n",
      "333 thank you\n",
      "413 last year\n",
      "900 ice cream\n",
      "1077 cochlear implant\n",
      "1097 don't want\n",
      "1108 fall in love\n",
      "1121 gallaudet\n",
      "1124 give up\n",
      "1131 hearing aid\n",
      "1136 high school\n",
      "1226 sign language\n",
      "1282 a lot\n",
      "1298 all day\n",
      "1344 come here\n",
      "1406 french fries\n",
      "1417 grow up\n",
      "1420 hard of hearing\n",
      "1475 pay attention\n",
      "1477 peanut butter\n",
      "1512 remote control\n",
      "1546 sore throat\n",
      "1547 south america\n",
      "1563 take turns\n",
      "1564 take up\n",
      "1584 united states\n",
      "1588 wake up\n",
      "1591 wash face\n",
      "1673 common sense\n",
      "1710 dining room\n",
      "1735 every monday\n",
      "1736 every tuesday\n",
      "1756 from now on\n",
      "1761 get up\n",
      "1769 hang up\n",
      "1772 heart attack\n",
      "1777 hot dog\n",
      "1799 last week\n",
      "1809 little bit\n",
      "1811 look at\n",
      "1812 look for\n",
      "1833 new york\n",
      "1836 not yet\n",
      "1866 polar bear\n",
      "1891 put off\n",
      "1972 toilet paper\n",
      "1989 vice president\n",
      "Number error words for representation: 47\n"
     ]
    }
   ],
   "source": [
    "# Get word_embedding \n",
    "word_embeddings_wlasl = {}\n",
    "i = 0\n",
    "error_word = 0\n",
    "for word in list_word:\n",
    "    try:\n",
    "        word_embeddings_wlasl[word] = model.get_vector(word)\n",
    "    except:\n",
    "        # If word not in vocabulary\n",
    "        print('{} {}'.format(i,word))\n",
    "        word_embeddings_wlasl[word] = np.random.rand(300)\n",
    "        error_word +=1\n",
    "    i+=1\n",
    "print(\"Number error words for representation: {}\".format(error_word))\n",
    "\n",
    "# Store word_embeddings_wlasl in .pkl file\n",
    "with open('./word_embeddings_wlasl.pkl', mode='wb') as f:\n",
    "    pickle.dump(word_embeddings_wlasl,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "[ 3.700e-03 -1.681e-01  2.826e-01 -9.630e-02 -8.400e-03 -4.790e-02\n",
      "  2.733e-01 -3.540e-02 -5.100e-03 -8.440e-02  2.670e-02 -1.750e-02\n",
      " -1.460e-02  4.980e-02 -4.000e-03  1.730e-01 -1.054e-01  7.200e-02\n",
      " -9.100e-03  1.802e-01 -1.170e-02 -1.339e-01 -2.980e-02 -2.870e-02\n",
      "  9.090e-02  1.005e-01  7.790e-02 -2.930e-02  3.890e-02 -8.010e-02\n",
      "  1.740e-02  3.950e-02  2.690e-02  9.050e-02  5.870e-02 -6.100e-03\n",
      "  4.400e-03 -7.330e-02  1.110e-02  2.073e-01 -9.390e-02 -1.746e-01\n",
      "  4.200e-02  1.710e-02 -8.280e-02  1.846e-01  9.180e-02  1.190e-02\n",
      " -4.630e-02 -1.430e-02  5.240e-02 -2.910e-02  1.810e-02  1.769e-01\n",
      " -4.470e-02  6.680e-02 -9.470e-02 -6.130e-02 -3.883e-01  7.860e-02\n",
      " -1.270e-02 -2.189e-01  6.070e-02  4.310e-02  1.157e-01 -1.000e-04\n",
      " -3.400e-02 -1.330e-02 -5.200e-03 -1.822e-01 -7.720e-02 -2.850e-02\n",
      "  4.790e-02 -4.580e-02  8.850e-02  2.340e-02  8.400e-03 -6.560e-02\n",
      "  7.020e-02  8.710e-02  1.225e-01 -4.510e-02 -1.448e-01  6.120e-02\n",
      " -7.960e-02  6.810e-02  9.830e-02  4.210e-02  4.330e-02 -6.800e-03\n",
      " -1.450e-02  1.330e-02 -5.260e-02  9.200e-03 -3.400e-03  4.023e-01\n",
      " -1.880e-02 -4.140e-02 -3.180e-02  8.920e-02  9.060e-02 -2.900e-02\n",
      "  4.190e-02  9.500e-02 -5.910e-02  4.300e-03  6.800e-03 -6.960e-02\n",
      "  4.380e-02 -1.344e-01 -1.083e-01 -6.330e-02  1.820e-02  1.246e-01\n",
      "  9.130e-02 -1.408e-01 -6.240e-02 -2.660e-02 -1.298e-01 -2.185e-01\n",
      " -1.066e-01  7.340e-02  2.780e-02  2.920e-02 -1.330e-02 -4.690e-02\n",
      "  1.491e-01  2.020e-02  3.410e-02  9.390e-02  3.090e-02 -1.480e-02\n",
      " -7.430e-02  2.090e-02 -1.260e-02 -1.840e-01  3.410e-02 -7.300e-03\n",
      " -7.880e-02 -5.440e-02  9.080e-02 -5.230e-02  5.880e-02 -1.490e-02\n",
      " -1.352e-01  9.450e-02 -1.880e-02 -1.286e-01  1.600e-03  3.220e-02\n",
      "  6.840e-02 -2.109e-01  7.240e-02 -1.150e-02 -1.231e-01 -4.930e-02\n",
      " -2.800e-02 -7.530e-02  9.930e-02  9.140e-02  3.700e-03  7.120e-02\n",
      " -3.910e-02 -2.980e-02  1.910e-02  2.460e-02 -1.024e-01 -1.536e-01\n",
      "  7.900e-03  3.360e-02  4.060e-02  1.423e-01  3.330e-02  5.500e-02\n",
      " -6.460e-02  4.230e-02 -8.260e-02  5.210e-02 -2.700e-02 -6.420e-02\n",
      " -4.370e-02  2.860e-02 -1.710e-02 -1.774e-01 -1.163e-01 -7.130e-02\n",
      " -7.350e-02  9.550e-02  5.820e-02  2.570e-02 -1.189e-01  8.500e-03\n",
      " -1.019e-01 -9.010e-02 -7.260e-02 -3.800e-03  1.782e-01  8.310e-02\n",
      "  3.590e-02 -1.950e-02 -1.850e-01 -4.750e-02  8.700e-02  2.830e-02\n",
      " -1.870e-02 -1.750e-02 -1.100e-03  1.057e-01 -2.540e-02  8.130e-02\n",
      " -2.542e-01 -9.730e-02 -1.133e-01 -1.172e-01 -3.400e-03  1.513e-01\n",
      "  3.620e-02 -3.700e-02 -1.324e-01  5.870e-02  6.050e-02 -2.530e-02\n",
      "  3.820e-02 -7.200e-02  8.650e-02  5.500e-03 -8.550e-02  8.060e-02\n",
      "  3.620e-02  6.320e-02  4.180e-02 -1.200e-02 -3.810e-02 -3.430e-02\n",
      "  1.160e-02 -6.540e-02 -5.350e-02  7.910e-02  6.150e-02 -1.054e-01\n",
      "  5.500e-03  8.800e-03 -1.620e-02 -3.100e-02  6.380e-02  8.130e-02\n",
      " -2.790e-02  6.630e-02 -9.330e-02 -1.144e-01  4.220e-02 -3.600e-02\n",
      " -3.190e-02 -7.800e-02  6.440e-02 -6.530e-02 -3.690e-02  1.482e-01\n",
      "  5.060e-02  5.820e-02 -3.440e-02 -7.980e-02 -9.120e-02  1.370e-02\n",
      "  5.570e-02 -4.360e-02  6.810e-02 -6.980e-02  1.999e-01  4.610e-02\n",
      "  2.900e-03 -5.220e-02 -6.190e-02 -5.200e-03 -2.160e-02 -6.930e-02\n",
      " -1.077e-01 -6.440e-02  3.160e-02 -5.220e-02  1.898e-01 -6.560e-02\n",
      "  1.116e-01 -1.163e-01  4.740e-02 -9.690e-02  7.310e-02 -2.273e-01\n",
      " -7.420e-02 -7.010e-02  8.840e-02  9.640e-02  6.240e-02  6.950e-02\n",
      "  5.400e-02  6.300e-02  5.950e-02 -1.533e-01  4.210e-02  3.330e-02]\n"
     ]
    }
   ],
   "source": [
    "# Load word_embedding\n",
    "import pickle\n",
    "with open(r'D:\\DATN\\project\\Pose-based-WLASL\\data\\wlasl_word_embeddings.pkl', mode='rb') as f:\n",
    "    word_embedding = pickle.load(f) \n",
    "print(word_embedding['eat'].shape)  \n",
    "print(word_embedding['eat'])   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
